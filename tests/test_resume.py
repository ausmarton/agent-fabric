"""Tests for resume_execute_task (Phase 12C P12-12).

Covers:
- resume_execute_task skips already-completed specialists.
- resume_execute_task seeds prev_finish_payload from checkpoint.
- resume_execute_task raises ValueError when no checkpoint is found.
- resume_execute_task raises ValueError when all specialists already complete.
- Full single-specialist resume (1 specialist, none completed).
- Partial sequential resume (2 specialists, first completed).
- Checkpoint is deleted after successful resume.
- resume_execute_task emits run_complete event.
"""
from __future__ import annotations

import time
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from agentic_concierge.application.execute_task import resume_execute_task
from agentic_concierge.config import ConciergeConfig
from agentic_concierge.config.schema import ModelConfig, SpecialistConfig
from agentic_concierge.domain import LLMResponse, RunId, Task, ToolCallRequest
from agentic_concierge.infrastructure.workspace.run_checkpoint import (
    RunCheckpoint,
    save_checkpoint,
)


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _make_config() -> ConciergeConfig:
    return ConciergeConfig(
        models={"quality": ModelConfig(base_url="http://x/v1", model="test-model")},
        specialists={
            "engineering": SpecialistConfig(
                description="Engineering",
                workflow="engineering",
                capabilities=["code_execution"],
            ),
            "research": SpecialistConfig(
                description="Research",
                workflow="research",
                capabilities=["systematic_review"],
            ),
        },
    )


def _finish_response(specialist_id: str = "engineering") -> LLMResponse:
    if specialist_id == "research":
        return LLMResponse(
            content=None,
            tool_calls=[ToolCallRequest(
                call_id="f0",
                tool_name="finish_task",
                arguments={
                    "executive_summary": "research done",
                    "key_findings": [],
                    "citations": [],
                    "gaps_and_future_work": [],
                },
            )],
        )
    return LLMResponse(
        content=None,
        tool_calls=[ToolCallRequest(
            call_id="f0",
            tool_name="finish_task",
            arguments={
                "summary": f"{specialist_id} done",
                "artifacts": [],
                "next_steps": [],
                "notes": "",
                "tests_verified": True,
            },
        )],
    )


def _tool_resp(call_id: str = "t0") -> LLMResponse:
    return LLMResponse(
        content=None,
        tool_calls=[ToolCallRequest(call_id=call_id, tool_name="list_files", arguments={})],
    )


def _make_checkpoint(
    workspace_root: str,
    run_id: str = "run-resume-test",
    specialist_ids: list[str] | None = None,
    completed_specialists: list[str] | None = None,
    payloads: dict | None = None,
) -> tuple[RunCheckpoint, str]:
    """Create and save a checkpoint. Returns (checkpoint, run_dir)."""
    sids = specialist_ids or ["engineering"]
    completed = completed_specialists or []
    run_dir = str(Path(workspace_root) / "runs" / run_id)
    Path(run_dir).mkdir(parents=True, exist_ok=True)
    Path(run_dir, "workspace").mkdir(exist_ok=True)

    cp = RunCheckpoint(
        run_id=run_id,
        run_dir=run_dir,
        workspace_path=str(Path(run_dir) / "workspace"),
        task_prompt="build a service",
        specialist_ids=sids,
        completed_specialists=completed,
        payloads=payloads or {},
        task_force_mode="sequential",
        model_key="quality",
        routing_method="orchestrator",
        required_capabilities=[],
        orchestration_plan=None,
        created_at=time.time(),
        updated_at=time.time(),
    )
    save_checkpoint(run_dir, cp)
    return cp, run_dir


def _make_mock_registry(specialist_id: str, responses: list[LLMResponse]):
    """Return a stub registry and chat client for the given specialist."""
    from agentic_concierge.infrastructure.specialists.base import BaseSpecialistPack

    pack = MagicMock(spec=BaseSpecialistPack)
    pack.specialist_id = specialist_id
    pack.system_prompt = f"You are {specialist_id}."
    pack.finish_tool_name = "finish_task"

    if specialist_id == "research":
        pack.finish_required_fields = ["executive_summary", "key_findings", "citations", "gaps_and_future_work"]
    else:
        pack.finish_required_fields = ["summary", "tests_verified", "artifacts", "next_steps"]

    pack.tool_definitions = [
        {
            "type": "function",
            "function": {
                "name": "list_files",
                "description": "List files",
                "parameters": {"type": "object", "properties": {}, "required": []},
            },
        },
        {
            "type": "function",
            "function": {
                "name": "finish_task",
                "description": "Finish",
                "parameters": {"type": "object", "properties": {
                    "summary": {"type": "string"},
                    "tests_verified": {"type": "boolean"},
                    "artifacts": {"type": "array", "items": {"type": "string"}},
                    "next_steps": {"type": "array", "items": {"type": "string"}},
                }, "required": ["summary", "tests_verified"]},
            },
        },
    ]
    pack.aopen = AsyncMock()
    pack.aclose = AsyncMock()
    pack.validate_finish_payload = MagicMock(return_value=None)

    async def _execute_tool(name, args):
        if name == "list_files":
            return {"files": []}
        return {"error": "unknown"}

    pack.execute_tool = _execute_tool

    registry = MagicMock()
    registry.get_pack.return_value = pack
    return registry


def _make_run_repository(run_dir: str):
    repo = MagicMock()
    repo.create_run.return_value = (RunId(value="unused"), run_dir, str(Path(run_dir) / "workspace"))
    repo.append_event = MagicMock()
    return repo


# ---------------------------------------------------------------------------
# Error cases
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_resume_raises_on_missing_checkpoint(tmp_path):
    """resume_execute_task raises ValueError when no checkpoint is found."""
    config = _make_config()
    with pytest.raises(ValueError, match="No checkpoint"):
        await resume_execute_task(
            "nonexistent-run",
            str(tmp_path),
            chat_client=MagicMock(),
            run_repository=MagicMock(),
            specialist_registry=MagicMock(),
            config=config,
        )


@pytest.mark.asyncio
async def test_resume_raises_when_already_complete(tmp_path):
    """resume_execute_task raises ValueError when all specialists are completed."""
    _make_checkpoint(
        str(tmp_path),
        specialist_ids=["engineering"],
        completed_specialists=["engineering"],
        payloads={"engineering": {"action": "final", "summary": "done", "tests_verified": True}},
    )
    config = _make_config()
    with pytest.raises(ValueError, match="already complete"):
        await resume_execute_task(
            "run-resume-test",
            str(tmp_path),
            chat_client=MagicMock(),
            run_repository=MagicMock(),
            specialist_registry=MagicMock(),
            config=config,
        )


# ---------------------------------------------------------------------------
# Successful resume
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_resume_single_specialist_completes(tmp_path):
    """Full resume: single specialist with no completed work â†’ runs and returns payload."""
    cp, run_dir = _make_checkpoint(str(tmp_path), specialist_ids=["engineering"])
    config = _make_config()
    registry = _make_mock_registry("engineering", [_tool_resp(), _finish_response("engineering")])
    repo = _make_run_repository(run_dir)

    responses = [_tool_resp(), _finish_response("engineering")]
    resp_iter = iter(responses)

    async def mock_chat(*args, **kwargs):
        return next(resp_iter)

    chat_client = MagicMock()
    chat_client.chat = mock_chat

    result = await resume_execute_task(
        "run-resume-test",
        str(tmp_path),
        chat_client=chat_client,
        run_repository=repo,
        specialist_registry=registry,
        config=config,
        max_steps=10,
    )

    assert result.payload.get("action") == "final"
    assert result.payload.get("summary") == "engineering done"


@pytest.mark.asyncio
async def test_resume_skips_completed_specialist(tmp_path):
    """When engineering is already completed, resume runs only research."""
    cp, run_dir = _make_checkpoint(
        str(tmp_path),
        specialist_ids=["engineering", "research"],
        completed_specialists=["engineering"],
        payloads={"engineering": {"action": "final", "summary": "eng done", "tests_verified": True}},
    )
    config = _make_config()
    # Only the research pack should be called
    registry = _make_mock_registry("research", [_tool_resp(), _finish_response("research")])
    repo = _make_run_repository(run_dir)

    responses = [_tool_resp(), _finish_response("research")]
    resp_iter = iter(responses)

    async def mock_chat(*args, **kwargs):
        return next(resp_iter)

    chat_client = MagicMock()
    chat_client.chat = mock_chat

    result = await resume_execute_task(
        "run-resume-test",
        str(tmp_path),
        chat_client=chat_client,
        run_repository=repo,
        specialist_registry=registry,
        config=config,
        max_steps=10,
    )

    # registry.get_pack was called for research (and maybe engineering, but engineering is skipped)
    # Result should come from research
    assert result.specialist_id == "engineering"  # primary = first
    assert result.specialist_ids == ["engineering", "research"]


@pytest.mark.asyncio
async def test_resume_deletes_checkpoint_on_success(tmp_path):
    """Checkpoint is deleted after a successful resume."""
    cp, run_dir = _make_checkpoint(str(tmp_path), specialist_ids=["engineering"])
    config = _make_config()
    registry = _make_mock_registry("engineering", [])
    repo = _make_run_repository(run_dir)

    responses = [_tool_resp(), _finish_response("engineering")]
    resp_iter = iter(responses)

    async def mock_chat(*args, **kwargs):
        return next(resp_iter)

    chat_client = MagicMock()
    chat_client.chat = mock_chat

    await resume_execute_task(
        "run-resume-test",
        str(tmp_path),
        chat_client=chat_client,
        run_repository=repo,
        specialist_registry=registry,
        config=config,
        max_steps=10,
    )

    assert not (Path(run_dir) / "checkpoint.json").exists()


@pytest.mark.asyncio
async def test_resume_emits_run_complete_event(tmp_path):
    """resume_execute_task emits a run_complete event via run_repository.append_event."""
    cp, run_dir = _make_checkpoint(str(tmp_path), specialist_ids=["engineering"])
    config = _make_config()
    registry = _make_mock_registry("engineering", [])
    repo = _make_run_repository(run_dir)

    responses = [_tool_resp(), _finish_response("engineering")]
    resp_iter = iter(responses)

    async def mock_chat(*args, **kwargs):
        return next(resp_iter)

    chat_client = MagicMock()
    chat_client.chat = mock_chat

    await resume_execute_task(
        "run-resume-test",
        str(tmp_path),
        chat_client=chat_client,
        run_repository=repo,
        specialist_registry=registry,
        config=config,
        max_steps=10,
    )

    # Check that run_complete event was appended
    event_kinds = [call.args[1] for call in repo.append_event.call_args_list]
    assert "run_complete" in event_kinds
